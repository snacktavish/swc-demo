{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pickle\n",
      "import numpy"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "> How about this for a measure of goodness-of-fit:\n",
      ">\n",
      "> Every polymorphism has to be in one of seven categories:\n",
      "> 1. Segregating among all three lineages\n",
      "> 2. Segregating in Indian and European lineages\n",
      "> 3. Segregating in Indian and African lineages\n",
      "> 4. Segregating in European and African lineages\n",
      "> 5. Segregating only in Indian lineage\n",
      "> 6. Segregating only in African lineage\n",
      "> 7. Segregating only in European lineage\n",
      ">\n",
      "> The total should reach 100%.\n",
      ">\n",
      "> If we use percentages in each category, and take the absolute difference of the empirical percentages and the model percentage for each, and sum them, then that should be a straightforward quantitative measure of the degree of match. You could even make a table (example data only):\n",
      ">\n",
      "> Percent of polymorphisms segregating within each group\n",
      "> 1 2 3 4 5 6 7 Sum of Absolute Differences\n",
      ">\n",
      "> Empirical 20 10 10 10 15 15 20\n",
      "> Model 1 18 12 15  5 20 15 15 24%\n",
      "> Model 2 30 5 5 15 10 25 10 50%\n",
      "> etc.\n",
      ">\n",
      "> The lower the sum of absolute differences, the closer the fit. Also, the table makes it easy to see where the deviations come from. The table could be a supplement, with the sum of absolute differences reported in the main text. What do you think about that?\n",
      ">\n",
      "> For the other question, you might just point out that our previous paper already explored the use of allelic frequency distributions (in the form of Structure analyses) to look at demographic history, and that you don't want to duplicate these analyses here?\n",
      ">\n",
      "> David"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def popdiv_geno(garr,persamp=25):\n",
      "    assert(garr.shape[0]==75)\n",
      "    pop_split={}\n",
      "    poly_dict={}\n",
      "    maf_dict={}\n",
      "    pop_split['eur']=garr[:persamp,:]\n",
      "    pop_split['afr']=garr[persamp:2*persamp,:]\n",
      "    pop_split['indi']=garr[2*persamp:,:]\n",
      "    maf_dict['all']=(garr.sum(axis=0))/len(garr[:,1])\n",
      "    for item in pop_split:\n",
      "        sumz=pop_split[item].sum(axis=0)\n",
      "        perc=sumz/float(len(pop_split[item]))\n",
      "        maf_dict[item]=perc\n",
      "        poly_dict[item]=[]\n",
      "        for i,val in enumerate(perc):\n",
      "            if 0.05 < val < 1.95:\n",
      "                poly_dict[item].append(i)\n",
      "    return(maf_dict,poly_dict)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def venn_poly_perc(poly_dict,output):\n",
      "    A=len(set(poly_dict['eur'])- (set(poly_dict['indi'])|set(poly_dict['afr']) ))\n",
      "    B=len(set(poly_dict['afr'])- (set(poly_dict['indi'])|set(poly_dict['eur']) ))\n",
      "    C=len(set(poly_dict['indi'])- (set(poly_dict['eur'])|set(poly_dict['afr']) ))\n",
      "    AB=len((set(poly_dict['eur']) & set(poly_dict['afr']))-(set(poly_dict['eur'])&(set(poly_dict['afr'])&set(poly_dict['indi']))))\n",
      "    AC=len((set(poly_dict['eur'])&set(poly_dict['indi']))-(set(poly_dict['eur'])&(set(poly_dict['afr'])&set(poly_dict['indi']))))\n",
      "    BC=len((set(poly_dict['afr'])&set(poly_dict['indi']))-(set(poly_dict['eur'])&(set(poly_dict['afr'])&set(poly_dict['indi']))))\n",
      "    ABC=len(set(poly_dict['eur'])&(set(poly_dict['afr'])&set(poly_dict['indi'])))\n",
      "    tot=sum([A,B,C,AB,AC,BC,ABC])*1.0\n",
      "    return([A/tot,B/tot,C/tot,AB/tot,AC/tot,BC/tot,ABC/tot])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 59
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#empir=venn_counts['randr']\n",
      "venn_counts={}\n",
      "diffs={}\n",
      "for i in range(0,5):\n",
      "    ii=str(i)\n",
      "    subsamp=pickle.load(open(\"FinalSub%i.p\"%i,'rb'))\n",
      "    venn_counts[i]={}\n",
      "    diffs[i]={}\n",
      "    empir=venn_poly_perc(popdiv_geno(subsamp['rand']['r'])[1],'randr')\n",
      "    for groups in subsamp:\n",
      "        for types in subsamp[groups]:  \n",
      "            venn_counts[i][groups+types]=(venn_poly_perc(popdiv_geno(subsamp[groups][types])[1],groups+types))\n",
      "           # diffs[i][groups+types]=numpy.absolute(numpy.array(venn_poly_perc(popdiv_geno(subsamp[groups][types])[1],groups+types))-numpy.array(empir))\n",
      "            diffs[i][groups+types]=numpy.array(venn_poly_perc(popdiv_geno(subsamp[groups][types])[1],groups+types))-numpy.array(empir)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 131
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "meanz={}\n",
      "for groups in subsamp:\n",
      "        for types in subsamp[groups]:  \n",
      "            mn=numpy.mean([diffs[i][groups+types] for i in range(0,5)],axis=0)\n",
      "            sd=numpy.std(sum([diffs[i][groups+types] for i in range(0,5)]))\n",
      "            sdd=numpy.std([diffs[i][groups+types] for i in range(0,5)],axis=0)\n",
      "            meanz[groups+types]=(mn,sum(mn),sd,sdd)\n",
      "            \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 132
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sdd"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 133,
       "text": [
        "array([ 0.0134085 ,  0.00720292,  0.00794838,  0.01685404,  0.00300412,\n",
        "        0.0054547 ,  0.01279895])"
       ]
      }
     ],
     "prompt_number": 133
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fi=open(\"Goodness_offit1.csv\",'w')\n",
      "pm=u\"\\u00B1\".encode('utf8')\n",
      "fi.write(\",I,II,III\\n\")\n",
      "for lett in ['a','b','w']:\n",
      "   fi.write(lett)\n",
      "   for bias in ['rand','Eur','Poly'] :\n",
      "        fi.write(\",{mean:.2f}{pm}{sd:.2f}\".format(pm=pm,mean=meanz[bias+lett][1],sd=meanz[bias+lett][2]))\n",
      "   fi.write(\"\\n\")\n",
      "for i in range(0,7):\n",
      "   fi.write(['Europe only','Africa only','India only','Europe and Africa','Europe and India','Africa and India','All regions'][i]+'\\n') \n",
      "   for lett in ['a','b','w']:\n",
      "      fi.write(lett)\n",
      "      for bias in ['rand','Eur','Poly'] :\n",
      "          fi.write(\",{mean:.2f}{pm}{sd:.2f}\".format(pm=pm,mean=meanz[bias+lett][0][i],sd=meanz[bias+lett][3][i]))\n",
      "      fi.write(\"\\n\")        \n",
      "  \n",
      "fi.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 134
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The fit measure I described below is for Fig 3. For Fig 4, how about the following:\n",
      "\n",
      "First, we need to say somewhere on Fig 4 that the horizontal axis is PC1 and the vertical axis is PC2 (or label them). \n",
      "\n",
      "Second, since the polarity of the axes is arbitrary (which end is negative or positive), put all the analyses in the same orientation (they are shown that way now, but the axes are labeled variably as to which end is negative\u2026why not label them all the same? Just say that Europe scores are positive, and adjust all accordingly.).\n",
      "\n",
      "Then, calculate deviations as follows, with the following values as the centroids of the respective groups, measured in standard deviation units:\n",
      "\n",
      "Europe India Africa Sum of absolute deviations\n",
      "PC1 PC2 PC1 PC2 PC1 PC2\n",
      "Empirical 0.12 0.08 -0.12 0.08 0.00 -0.16 NA\n",
      "Model 1 0.09 0.10 -0.09 0.00 0.09 -0.10 0.31\n",
      "Model 2 0.11 0.08 -0.12 0.07 0.01 -0.15 0.04\n",
      "etc.\n",
      "\n",
      "On May 21, 2014, at 3:02 PM, David Hillis wrote:\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vals_dict={}\n",
      "vals_dict['PC1']={}\n",
      "vals_dict['PC2']={}\n",
      "vals_dict['fst1-2']={}\n",
      "vals_dict['fst1-3']={}\n",
      "vals_dict['fst2-3']={}\n",
      "subsamp=pickle.load(open(\"FinalSub0.p\",'rb'))\n",
      "\n",
      "for item in vals_dict:\n",
      "   for group in subsamp:\n",
      "        for types in subsamp[group]:\n",
      "            vals_dict[item][(group+types)]=[]\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(vals_dict['PC1']['Eurw'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 10,
       "text": [
        "5"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "##Fit for centroids..\n",
      "\n",
      "for i in range(0,5):\n",
      "    ii=str(i)\n",
      "    subsamp=pickle.load(open(\"FinalSub%i.p\"%i,'rb'))\n",
      "    for groups in subsamp:\n",
      "        for types in subsamp[groups]:\n",
      "            gpfi=open(\"./Genepop/sources/%s.txt.MIG\"%(groups+types+ii)).readlines()\n",
      "            vals_dict['fst1-2'][groups+types].append(float(gpfi[3]))\n",
      "            vals_dict['fst1-3'][groups+types].append(float(gpfi[4].split()[0]))\n",
      "            vals_dict['fst2-3'][groups+types].append(float(gpfi[4].split()[1]))\n",
      "            evalfi=[]\n",
      "            fi=open(\"%s.eval\"%(groups+types+ii))\n",
      "            for item in fi:\n",
      "                evalfi.append(float(item))\n",
      "            vals_dict['PC1'][groups+types].append(evalfi[0]/sum(evalfi))\n",
      "            vals_dict['PC2'][groups+types].append(evalfi[1]/sum(evalfi))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy\n",
      "def dist((x,y),(a,b)):\n",
      "    return numpy.sqrt((x-a)*(x-a) + (y-b)*(y-b))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 58
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "i=0\n",
      "coords={}\n",
      "centroids={}\n",
      "dists={}    \n",
      "\n",
      "for groups in ['a','b','w','r']:\n",
      "      for types in [\"rand\",\"Eur\",\"Poly\"]:\n",
      "            dists[types+groups]={}\n",
      "\n",
      "for i in range(5):\n",
      "    coords[i]={}\n",
      "    centroids[i]={}\n",
      "    for groups in ['a','b','w','r']:\n",
      "      for types in [\"rand\",\"Eur\",\"Poly\"]:\n",
      "            try:\n",
      "              fi=open(\"{:s}{:s}{}.evec\".format(types,groups,i)).readlines()\n",
      "            except: break\n",
      "            evec_dict={}\n",
      "            loc_dict={}\n",
      "            locset=set()\n",
      "            coords[i][types+groups]={}\n",
      "            centroids[i][types+groups]={}\n",
      "            for lin in fi[1:]:\n",
      "                lii=lin.split()\n",
      "                evec_dict[lii[0]]=lii[1:-1]\n",
      "                loc_dict[lii[0]]=lii[-1]\n",
      "                locset.add(lii[-1])\n",
      "            for region in locset:\n",
      "                coords[i][types+groups][region]={}\n",
      "                coords[i][types+groups][region]['PC1']=[]\n",
      "                coords[i][types+groups][region]['PC2']=[]\n",
      "            for ind in evec_dict:\n",
      "                coords[i][types+groups][loc_dict[ind]]['PC1'].append(float(evec_dict[ind][0]))\n",
      "                coords[i][types+groups][loc_dict[ind]]['PC2'].append(float(evec_dict[ind][1]))\n",
      "            for region in locset:\n",
      "                centroids[i][types+groups][region]=(sum(coords[i][types+groups][region]['PC1'])/25,sum(coords[i][types+groups][region]['PC2'])/25)\n",
      "            for dif in [('Europe', 'Africa'),('Europe', 'India'),('Africa','India')]:\n",
      "                if dif not in dists[types+groups]:\n",
      "                    dists[types+groups][dif]=[]\n",
      "                dists[types+groups][dif].append(dist(centroids[i][types+groups][dif[0]],centroids[i][types+groups][dif[1]]))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 93
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for i in range(5):\n",
      "    print(centroids[i][types+groups][region])\n",
      "\n",
      "numpy.std(dists[types+groups][dif])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(-0.035288, 0.15771600000000002)\n",
        "(-0.03336800000000001, 0.158316)\n",
        "(-0.035804, 0.15736799999999998)\n",
        "(-0.03385200000000001, 0.158008)\n",
        "(-0.03278400000000001, 0.15831199999999998)\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 133,
       "text": [
        "0.00012684964681581592"
       ]
      }
     ],
     "prompt_number": 133
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "numpy.abs(numpy.array(dists['randr'][dif])-numpy.array(dists[types+groups][dif]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 139,
       "text": [
        "array([ 0.00103251,  0.00132376,  0.00090726,  0.00128547,  0.00076254])"
       ]
      }
     ],
     "prompt_number": 139
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Make table\n",
      "devs={}\n",
      "for groups in ['a','b','w']:\n",
      "      for types in [\"rand\",\"Eur\",\"Poly\"]: \n",
      "            devs[types+groups]={}\n",
      "            for dif in [('Europe', 'Africa'),('Europe', 'India'),('Africa','India')]:\n",
      "                devarr=numpy.abs(numpy.array(dists['randr'][dif])-numpy.array(dists[types+groups][dif]))\n",
      "                devs[types+groups][dif]=(numpy.mean(devarr),numpy.std(devarr), devarr)\n",
      "\n",
      "totdevs={}\n",
      "for sub in devs:\n",
      "    totarr=numpy.array([0,0,0,0,0])\n",
      "    for dif in devs[sub]:\n",
      "        totarr=totarr+devs[sub][dif][2]\n",
      "    totdevs[sub]=totarr"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 146
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fi=open(\"devs_table.csv\",'w')\n",
      "pm=u\"\\u00B1\".encode('utf8')\n",
      "fi.write(\",,I,,,II,,,III\\n\")\n",
      "for lett in ['a','b','w']:\n",
      "   fi.write(\",,Eur,Afr,,Eur,Afr,,Eur,Afr\\n\")\n",
      "   fi.write(lett)\n",
      "   for bias in ['rand','Eur','Poly'] :\n",
      "        fi.write(\",Afr,{mean:.5f}{pm}{sd:.4f},\".format(pm=pm,mean=devs[bias+lett][('Europe', 'Africa')][0],sd=devs[bias+lett][('Europe', 'Africa')][1]))\n",
      "   fi.write(\"\\n\")\n",
      "   for bias in ['rand','Eur','Poly'] :\n",
      "      fi.write(\",Ind,{mean:.5f}{pm}{sd:.4f},{mean2:.5f}{pm}{sd2:.4f}\".format(pm=pm,mean=devs[bias+lett][('Europe', 'India')][0],sd=devs[bias+lett][('Europe', 'India')][1],mean2=devs[bias+lett][('Africa', 'India')][0],sd2=devs[bias+lett][('Europe', 'India')][1]))\n",
      "   fi.write(\"\\n\")\n",
      "   for bias in ['rand','Eur','Poly'] :      \n",
      "      fi.write(\",,total deviation = {:.3f}{}{:.4f},\".format(numpy.mean(totdevs[bias+lett]),pm,numpy.std(totdevs[bias+lett])))\n",
      "   fi.write(\"\\n\")  \n",
      "fi.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 150
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from subprocess import Popen, PIPE"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "i=0\n",
      "sumSS={}\n",
      "for groups in ['a','b','w']:\n",
      "      for types in [\"rand\",\"Eur\",\"Poly\"]:\n",
      "            sumSS[types+groups]=[]\n",
      "\n",
      "for i in range(5):\n",
      "    for groups in ['a','b','w']:\n",
      "      for types in [\"rand\",\"Eur\",\"Poly\"]:\n",
      "            p = Popen([\"Rscript\",\"Procrustes.R\",\"randr{}.evec\".format(i),\"{:s}{:s}{}.evec\".format(types,groups,i)], stdin=PIPE, stdout=PIPE, stderr=PIPE)\n",
      "            output, err = p.communicate(b\"input data that is passed to subprocess' stdin\")\n",
      "            sumSS[types+groups].append(float(output.split()[1]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 31
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "err"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 30,
       "text": [
        "\"Loading required package: permute\\nLoading required package: lattice\\nThis is vegan 2.0-10\\nError in length(choices) : 'choices' is missing\\nCalls: run_proc -> procrustes -> scores -> scores.default\\nExecution halted\\n\""
       ]
      }
     ],
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sumSS"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 32,
       "text": [
        "{'Eura': [0.001824655, 1.540813e-05, 0.00140543, 0.0008336814, 0.000566252],\n",
        " 'Eurb': [0.0002792503, 0.0007193189, 0.0001309001, 0.0003699261, 0.001055737],\n",
        " 'Eurw': [0.001443478, 0.0002519448, 0.0006845798, 6.630119e-05, 0.001745953],\n",
        " 'Polya': [0.002169974, 0.0001746169, 0.002788975, 0.0002267587, 0.0008891341],\n",
        " 'Polyb': [0.0009410214, 0.000147396, 0.002479988, 0.0005297464, 0.0002008656],\n",
        " 'Polyw': [0.001977659, 0.0001135148, 0.001116289, 0.0008412274, 0.0004292148],\n",
        " 'randa': [0.0007575746, 0.001350064, 0.002586168, 0.0001431673, 0.001378447],\n",
        " 'randb': [0.0005184712,\n",
        "  8.029766e-06,\n",
        "  0.003591702,\n",
        "  0.0005545404,\n",
        "  0.0006634045],\n",
        " 'randw': [0.001890199, 0.0007560302, 0.001539421, 0.0001519172, 0.0004451868]}"
       ]
      }
     ],
     "prompt_number": 32
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy\n",
      "meanSS={}\n",
      "for types in ['a','b','w']:\n",
      "      for groups in [\"rand\",\"Eur\",\"Poly\"]:\n",
      "            mn=numpy.mean(sumSS[groups+types])\n",
      "            sdd=numpy.std(sumSS[groups+types])\n",
      "            meanSS[groups+types]=(mn,sdd)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 34
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "meanSS"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 35,
       "text": [
        "{'Eura': (0.00092908530599999993, 0.00063316747223455638),\n",
        " 'Eurb': (0.00051102647999999996, 0.00033413196867452475),\n",
        " 'Eurw': (0.00083845135800000013, 0.00065627718970619046),\n",
        " 'Polya': (0.00124989174, 0.0010534222249671754),\n",
        " 'Polyb': (0.00085980347999999995, 0.000858167285870736),\n",
        " 'Polyw': (0.000895581, 0.00064057538305274274),\n",
        " 'randa': (0.0012430841799999999, 0.00080983684994597371),\n",
        " 'randb': (0.0010672295732000001, 0.001282335829294697),\n",
        " 'randw': (0.00095655083999999992, 0.00065735432046660981)}"
       ]
      }
     ],
     "prompt_number": 35
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Out[35]:\n",
      "\n",
      "{'Eura': (0.044893979999999993, 0.0063949749765577655),\n",
      " 'Eurb': (0.036298586000000001, 0.0032653443724458843),\n",
      " 'Eurw': (0.043704038000000001, 0.0023257560977144624),\n",
      " 'Polya': (0.037898969999999997, 0.0023608279596531393),\n",
      " 'Polyb': (0.042829223999999999, 0.0042006095007729536),\n",
      " 'Polyw': (0.049587303999999999, 0.0076079012334982384),\n",
      " 'randa': (0.075285175999999995, 0.008319247573271512),\n",
      " 'randb': (0.058723057999999995, 0.0078989842795650633),\n",
      " 'randw': (0.049963687999999992, 0.007584062495081114)}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "SyntaxError",
       "evalue": "invalid syntax (<ipython-input-12-b716fc34a5a0>, line 1)",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-12-b716fc34a5a0>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    Out[35]:\u001b[0m\n\u001b[1;37m           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fi=open(\"Goodness_offit_prc.csv\",'w')\n",
      "pm=u\"\\u00B1\".encode('utf8')\n",
      "fi.write(\",I,II,III\\n\")\n",
      "for lett in ['a','b','w']:\n",
      "   fi.write(lett)\n",
      "   for bias in ['rand','Eur','Poly'] :\n",
      "        fi.write(\",{mean:.3f}{pm}{sd:.3f}\".format(pm=pm,mean=meanSS[bias+lett][0],sd=meanSS[bias+lett][1]))\n",
      "   fi.write(\"\\n\")\n",
      "fi.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "output"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 21,
       "text": [
        "'[1] 0.08246376\\n'"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "err"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 18,
       "text": [
        "'Loading required package: permute\\nLoading required package: lattice\\nThis is vegan 2.0-10\\n'"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}